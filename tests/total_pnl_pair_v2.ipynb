{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0c61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import text\n",
    "import matplotlib.pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "import redis\n",
    "import json\n",
    "\n",
    "account = 'fund2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f62e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH REDIS TRADESHEET AND PREPROCESSING\n",
    "import json\n",
    "import pandas as pd\n",
    "import redis\n",
    "\n",
    "h = 'localhost'\n",
    "p = 6379\n",
    "r = redis.Redis(host=h, port=p)\n",
    "\n",
    "def getRedis(param):\n",
    "    try:\n",
    "        v = r.get(param)\n",
    "        val = json.loads(v)\n",
    "        return val\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "redis_tradesheet = getRedis(f\"{account}_tradesheet\")\n",
    "redis_tradesheet = pd.DataFrame(redis_tradesheet['tradeslist'])\n",
    "redis_tradesheet[['leg_1','leg_2']] = redis_tradesheet['pair'].str.split('_', expand=True)\n",
    "redis_tradesheet.to_csv(f\"{account}_tradesheet.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179bdd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 08:00:54\n",
      "2025-09-28 23:40:54\n"
     ]
    }
   ],
   "source": [
    "# FIND EARLIEST DATETIME\n",
    "earliest_datetime = redis_tradesheet.iat[0, redis_tradesheet.columns.get_loc(\"entry_dt\")]\n",
    "earliest_datetime = str(earliest_datetime.partition(\".\")[0])\n",
    "latest_datetime = redis_tradesheet.iat[-1, redis_tradesheet.columns.get_loc(\"exit_dt\")]\n",
    "latest_datetime = str(latest_datetime.partition(\".\")[0])\n",
    "print(earliest_datetime)\n",
    "print(latest_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8848348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCHING SQL TRADES\n",
    "def get_account_data(account):\n",
    "    conn = db.create_engine('mysql+mysqldb://247team:password@192.168.50.238:3306/trades')\n",
    "    query = f\"SELECT * FROM {account};\"\n",
    "    df = pd.read_sql_query(text(query), conn.connect())\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df['realizedPnl'] = df['realizedPnl'].astype(float)\n",
    "    df['commission'] = df['commission'].astype(float)\n",
    "    df['realizedPnl'] = df['realizedPnl'] - df['commission']\n",
    "    df = df.loc[earliest_datetime:latest_datetime]\n",
    "    df['account'] = account\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b08d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING SQL TRADES\n",
    "raw_data = []\n",
    "raw_data.append(get_account_data(account))\n",
    "sql_trades = pd.concat(raw_data)\n",
    "sql_trades.to_csv(f\"{account}_sql_trades.csv\", index=True)\n",
    "# print(sql_trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc39568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGN PAIR_NAME IN SQL TRADES\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "def _snake_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def snake(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s)\n",
    "        s = re.sub(r\"\\W+\", \"_\", s)\n",
    "        s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "        return s.lower()\n",
    "    out = df.copy()\n",
    "    out.columns = [snake(c) for c in out.columns]\n",
    "    return out\n",
    "\n",
    "def assign_pair_names(\n",
    "    sql_trades: pd.DataFrame,\n",
    "    redis_tradesheet: pd.DataFrame,\n",
    "    price_decimals: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map (symbol, price) from Redis rows:\n",
    "      (leg0, entry_price_0), (leg1, entry_price_1), (leg0, exit_price_0), (leg1, exit_price_1)\n",
    "    onto SQL trades by exact symbol and rounded price, setting sql 'pair_name' = redis 'pair'.\n",
    "\n",
    "    Guarantees a 'time' COLUMN is present in the returned DataFrame (no dtype conversion).\n",
    "    \"\"\"\n",
    "    # --- SQL side: keep 'time' as a column, untouched ---\n",
    "    sql = sql_trades.copy()\n",
    "    if \"time\" not in sql.columns and sql.index.name is not None:\n",
    "        sql = sql.reset_index()  # bring index out as a column (whatever its name is)\n",
    "\n",
    "    sql = _snake_cols(sql)\n",
    "    if \"time\" not in sql.columns:\n",
    "        # Accept common aliases and rename to 'time' (no parsing)\n",
    "        for cand in (\"time\", \"timestamp\", \"transact_time\", \"transacttime\", \"event_time\", \"create_time\"):\n",
    "            if cand in sql.columns:\n",
    "                if cand != \"time\":\n",
    "                    sql = sql.rename(columns={cand: \"time\"})\n",
    "                break\n",
    "    if \"time\" not in sql.columns:\n",
    "        raise KeyError(\"No 'time' column found on sql_trades (even after reset/rename).\")\n",
    "\n",
    "    # --- Redis side: normalize & resolve leg columns ---\n",
    "    rds = _snake_cols(redis_tradesheet).rename(\n",
    "        columns={\"leg2\": \"leg_2\", \"leg1\": \"leg_1\", \"leg0\": \"leg_0\"}\n",
    "    )\n",
    "    leg0_col: Optional[str] = next((c for c in (\"leg_0\", \"leg0\", \"leg_1\") if c in rds.columns), None)\n",
    "    leg1_col: Optional[str] = next((c for c in (\"leg_1\", \"leg1\", \"leg_2\") if c in rds.columns and c != leg0_col), None)\n",
    "    if leg0_col is None or leg1_col is None:\n",
    "        raise KeyError(f\"Could not resolve leg columns. Found: {list(rds.columns)}\")\n",
    "\n",
    "    # --- Build mapping (pair, symbol, price) from the four Redis fields ---\n",
    "    parts: List[pd.DataFrame] = []\n",
    "    def add_part(symbol_col: str, price_col: str) -> None:\n",
    "        if price_col in rds.columns and symbol_col in rds.columns:\n",
    "            tmp = rds[[\"pair\", symbol_col, price_col]].rename(\n",
    "                columns={symbol_col: \"symbol\", price_col: \"price\"}\n",
    "            )\n",
    "            parts.append(tmp)\n",
    "\n",
    "    add_part(leg0_col, \"entry_price_0\")\n",
    "    add_part(leg1_col, \"entry_price_1\")\n",
    "    add_part(leg0_col, \"exit_price_0\")\n",
    "    add_part(leg1_col, \"exit_price_1\")\n",
    "    if not parts:\n",
    "        raise ValueError(\"No mapping parts could be constructed; check Redis columns.\")\n",
    "\n",
    "    mapping = pd.concat(parts, ignore_index=True)\n",
    "    mapping[\"symbol\"] = mapping[\"symbol\"].astype(str).str.strip()\n",
    "    mapping[\"price\"]  = pd.to_numeric(mapping[\"price\"], errors=\"coerce\")\n",
    "    mapping = mapping.dropna(subset=[\"symbol\", \"price\"])\n",
    "    mapping[\"price_r\"] = mapping[\"price\"].round(price_decimals)\n",
    "    mapping = mapping.drop_duplicates(subset=[\"symbol\", \"price_r\"], keep=\"first\")[[\"symbol\", \"price_r\", \"pair\"]]\n",
    "\n",
    "    # --- Prepare SQL side for join (keep 'time' as-is) ---\n",
    "    if \"symbol\" not in sql.columns or \"price\" not in sql.columns:\n",
    "        raise KeyError(\"SQL trades must have 'symbol' and 'price' columns.\")\n",
    "\n",
    "    out = sql.copy()\n",
    "    out[\"symbol\"] = out[\"symbol\"].astype(str).str.strip()\n",
    "    out[\"price\"]  = pd.to_numeric(out[\"price\"], errors=\"coerce\")\n",
    "    out[\"price_r\"] = out[\"price\"].round(price_decimals)\n",
    "\n",
    "    # --- Merge to assign pair_name; preserve 'time' and everything else ---\n",
    "    out = out.merge(mapping, how=\"left\", on=[\"symbol\", \"price_r\"])\n",
    "    if \"pair_name\" in sql.columns:\n",
    "        out[\"pair_name\"] = out[\"pair_name\"].where(out[\"pair_name\"].notna(), out[\"pair\"])\n",
    "    else:\n",
    "        out = out.rename(columns={\"pair\": \"pair_name\"})\n",
    "    out = out.drop(columns=[\"price_r\"])\n",
    "\n",
    "    # 'time' is present and unchanged (string if it was string).\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ec18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE CONSECUTIVE TRADES\n",
    "def merge_consecutive_trades(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Coalesce only adjacent rows with identical (symbol, side, position_side).\n",
    "    - earliest_time: first 'time' in the run (no parsing; uses existing order)\n",
    "    - latest_time: last 'time' in the run\n",
    "    - realized_pnl, commission: summed\n",
    "    - pair_name: first non-null/non-empty value in the run\n",
    "    - pair_name_conflict: True if >1 distinct non-empty values appear in the run\n",
    "    \"\"\"\n",
    "    # Keep original order; ensure expected columns exist (accept camelCase/space variants via your snake-casing)\n",
    "    d = df.copy()\n",
    "    # If 'time' lives on the index, bring it out (no parsing)\n",
    "    if \"time\" not in d.columns and d.index.name is not None:\n",
    "        d = d.reset_index()\n",
    "\n",
    "    # Normalize essential column names if needed (assuming you've already run your snake-casing)\n",
    "    rename_map = {}\n",
    "    if \"positionSide\" in d.columns: rename_map[\"positionSide\"] = \"position_side\"\n",
    "    if \"Position Side\" in d.columns: rename_map[\"Position Side\"] = \"position_side\"\n",
    "    if \"realizedPnl\" in d.columns:   rename_map[\"realizedPnl\"]   = \"realized_pnl\"\n",
    "    d = d.rename(columns=rename_map)\n",
    "\n",
    "    required: List[str] = [\"time\", \"symbol\", \"side\", \"position_side\", \"realized_pnl\", \"commission\"]\n",
    "    missing = [c for c in required if c not in d.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}. Available: {list(d.columns)}\")\n",
    "\n",
    "    # Numerics\n",
    "    d[\"realized_pnl\"] = pd.to_numeric(d[\"realized_pnl\"], errors=\"coerce\").fillna(0.0)\n",
    "    d[\"commission\"]   = pd.to_numeric(d[\"commission\"],   errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Run boundaries: a new run starts when any key changes vs previous row\n",
    "    keys = [\"symbol\", \"side\", \"position_side\"]\n",
    "    run_breaks = d[keys].ne(d[keys].shift(1)).any(axis=1)\n",
    "    run_id = run_breaks.cumsum()\n",
    "\n",
    "    def _pair_first_nonnull(s: pd.Series):\n",
    "        nonblank = s.dropna()\n",
    "        nonblank = nonblank[nonblank.astype(str).str.len() > 0]\n",
    "        return nonblank.iloc[0] if not nonblank.empty else np.nan\n",
    "\n",
    "    def _pair_conflict(s: pd.Series) -> bool:\n",
    "        nonblank = s.dropna()\n",
    "        nonblank = nonblank[nonblank.astype(str).str.len() > 0]\n",
    "        return nonblank.nunique(dropna=True) > 1\n",
    "\n",
    "    # Group by run and aggregate\n",
    "    out = (\n",
    "        d.assign(_run=run_id)\n",
    "         .groupby(keys + [\"_run\"], as_index=False, sort=False)\n",
    "         .agg(\n",
    "             earliest_time=(\"time\", \"first\"),\n",
    "             latest_time=(\"time\", \"last\"),\n",
    "             realized_pnl=(\"realized_pnl\", \"sum\"),\n",
    "             commission=(\"commission\", \"sum\"),\n",
    "             pair_name=(\"pair_name\", _pair_first_nonnull),\n",
    "             rows_merged=(\"time\", \"size\"),\n",
    "         )\n",
    "    )\n",
    "\n",
    "    # Optional conflict flag for visibility\n",
    "    conflicts = (\n",
    "        d.assign(_run=run_id)\n",
    "         .groupby(keys + [\"_run\"], sort=False)[\"pair_name\"]\n",
    "         .apply(_pair_conflict)\n",
    "         .reset_index(name=\"pair_name_conflict\")\n",
    "    )\n",
    "\n",
    "    out = out.merge(conflicts, on=keys + [\"_run\"], how=\"left\").drop(columns=\"_run\")\n",
    "\n",
    "    # Column order\n",
    "    out = out[\n",
    "        [\"earliest_time\", \"latest_time\", \"symbol\", \"side\", \"position_side\",\n",
    "         \"realized_pnl\", \"commission\", \"pair_name\", \"rows_merged\", \"pair_name_conflict\"]\n",
    "    ]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70bde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_trades_with_pairs = assign_pair_names(sql_trades, redis_tradesheet, price_decimals=3)\n",
    "sql_trades_with_pairs = sql_trades_with_pairs[['time','symbol', 'side', 'price', 'realized_pnl',\n",
    "                                               'commission', 'position_side', 'pair_name']]\n",
    "# sql_trades_with_pairs.to_csv(f\"{account}_sql_trades_pair.csv\", index=True)\n",
    "merged_trades = merge_consecutive_trades(sql_trades_with_pairs)[['earliest_time','latest_time','symbol','side',\n",
    "                                                                 'position_side','realized_pnl','commission','pair_name']]\n",
    "merged_trades.to_csv(f\"{account}_merged_trades.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f925b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'APTUSDT_AVAXUSDT': 45.563, 'AVAXUSDT_FILUSDT': -36.057, 'AVAXUSDT_DOTUSDT': 144.889, 'DOTUSDT_FILUSDT': -254.475, 'ATOMUSDT_FILUSDT': 113.7, 'UNIUSDT_VETUSDT': 36.315, 'ATOMUSDT_DOTUSDT': 142.507, 'LINKUSDT_LTCUSDT': -36.685, 'BTCUSDT_LTCUSDT': 75.392, 'ADAUSDT_LTCUSDT': -173.437, 'APTUSDT_FILUSDT': 87.177, 'LINKUSDT_VETUSDT': -5.628, 'ETHUSDT_FILUSDT': 37.821, 'ETHUSDT_NEARUSDT': 36.272, 'BNBUSDT_SOLUSDT': -146.16, 'undefined/ambiguous': 20}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Union\n",
    "import pandas as pd\n",
    "\n",
    "def net_pnl_by_pair(\n",
    "    merged_trades: pd.DataFrame,\n",
    "    pair_col: str = \"pair_name\",\n",
    "    commission_col: str = \"commission\",\n",
    "    round_decimals: int | None = None\n",
    ") -> Dict[str, Union[float, int]]:\n",
    "    \"\"\"\n",
    "    For each UNIQUE pair_name:\n",
    "      net = sum(realized_pnl) - sum(commission)\n",
    "    Returns a dict {pair_name: net}. Also adds {\"undefined/ambiguous\": <count>}\n",
    "    counting rows whose pair_name is null/empty or marked ambiguous.\n",
    "\n",
    "    Assumes columns exist: pair_name, realized_pnl (or realizedPnl), commission.\n",
    "    Does NOT parse/convert time; time not used here.\n",
    "    \"\"\"\n",
    "    df = merged_trades.copy()\n",
    "\n",
    "    # Resolve realized PnL column\n",
    "    rp_col = \"realized_pnl\" if \"realized_pnl\" in df.columns else (\n",
    "        \"realizedPnl\" if \"realizedPnl\" in df.columns else None\n",
    "    )\n",
    "    if rp_col is None:\n",
    "        raise KeyError(\"Expected 'realized_pnl' (or 'realizedPnl') column.\")\n",
    "    for c in (pair_col, commission_col):\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Missing required column: {c}\")\n",
    "\n",
    "    # Coerce numerics\n",
    "    df[rp_col] = pd.to_numeric(df[rp_col], errors=\"coerce\")\n",
    "    df[commission_col] = pd.to_numeric(df[commission_col], errors=\"coerce\")\n",
    "\n",
    "    # Clean pair names for grouping\n",
    "    pairs = df[pair_col].astype(\"string\")\n",
    "    pairs_clean = pairs.str.strip()\n",
    "\n",
    "    # Identify ambiguous/undefined\n",
    "    is_null_or_empty = pairs_clean.isna() | (pairs_clean == \"\")\n",
    "    is_ambiguous = pairs_clean.str.lower().str.startswith(\"ambiguous\", na=False) | (pairs_clean.str.lower() == \"undefined\")\n",
    "    valid_mask = ~(is_null_or_empty | is_ambiguous)\n",
    "\n",
    "    # Group valid pairs\n",
    "    df_valid = df.loc[valid_mask].copy()\n",
    "    if not df_valid.empty:\n",
    "        df_valid[\"_pair_clean\"] = pairs_clean[valid_mask]\n",
    "        grouped = df_valid.groupby(\"_pair_clean\", sort=False).agg(\n",
    "            rp_sum=(rp_col, \"sum\"),\n",
    "            cm_sum=(commission_col, \"sum\"),\n",
    "        )\n",
    "        net = grouped[\"rp_sum\"] - grouped[\"cm_sum\"]\n",
    "        result: Dict[str, Union[float, int]] = net.to_dict()\n",
    "    else:\n",
    "        result = {}\n",
    "\n",
    "    # Round if requested\n",
    "    if round_decimals is not None and result:\n",
    "        result = {k: (round(v, round_decimals) if isinstance(v, float) else v) for k, v in result.items()}\n",
    "\n",
    "    # Count undefined/ambiguous rows\n",
    "    undef_count = int((~valid_mask).sum())\n",
    "    result[\"undefined/ambiguous\"] = undef_count\n",
    "    return result\n",
    "\n",
    "tallied_results = net_pnl_by_pair(merged_trades, round_decimals=3)\n",
    "print(tallied_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afmonitor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
